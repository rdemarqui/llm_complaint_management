{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdemarqui/llm_complaint_management/blob/main/Tim%20Data%20Academy/tim_data_academy_mistral7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdXiJzMm7ewB"
      },
      "source": [
        "# Tagging Customer Feedback with LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ex4zf24D1-m"
      },
      "source": [
        "This study aims to explore the advanced LLM tool Mistral 7B to analyze a vast collection of complaints gathered from the website [reclameaqui.com.br](reclameaqui.com.br). Through this analysis, we intend to demonstrate the potential and effectiveness of LLMs in interpreting and handling large-scale user feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_s1QiOU98Rc"
      },
      "source": [
        "## Complaint Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZWO80q3-PsC"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSvJsvLbHoQp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cduSRhNh4VB5"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "data_url = 'https://raw.githubusercontent.com/rdemarqui/sbrc_2024/main/data/df_validation.xlsx'\n",
        "data = pd.read_excel(data_url, usecols=['problem', 'description'])\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAx7QfsGBLfN"
      },
      "source": [
        "### Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvxMuJuBOy7"
      },
      "outputs": [],
      "source": [
        "# Remove speciac characters, extra space and uncase all words\n",
        "data['description'] = data['description'].astype(str)\n",
        "data['description'] = data['description'].str.replace(r'<', ' ', regex=True)\n",
        "data['description'] = data['description'].str.replace(r'>', ' ', regex=True)\n",
        "data['description'] = data['description'].str.replace(r'#', ' ', regex=True)\n",
        "data['description'] = data['description'].str.replace(r'\\s{2,}', ' ', regex=True)\n",
        "data['description'] = data['description'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSHPHLv0JdLR"
      },
      "outputs": [],
      "source": [
        "# Modify mask made by reclame aqui\n",
        "def remove_mask(text):\n",
        "  return text.replace(\"[editado pelo reclame aqui]\", \"[mask]\")\n",
        "\n",
        "data['description'] = data['description'].apply(remove_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9DYRkEGdB5L"
      },
      "outputs": [],
      "source": [
        "# Remove description less than 3 characters\n",
        "data = data[data['description'].str.len() > 3].copy()\n",
        "df = data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlSuXm0N-APD"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trUhFZEarsl5"
      },
      "outputs": [],
      "source": [
        "# Show each problem category\n",
        "df['problem'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0gvCGaxsuBp"
      },
      "outputs": [],
      "source": [
        "# Show words statistics\n",
        "df['word_count'] = df['description'].str.split().str.len()\n",
        "df['text_len'] = df['description'].str.len()\n",
        "df['word_count'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMrNKNN4vdjj"
      },
      "outputs": [],
      "source": [
        "# Check correlation between word count and text lenght\n",
        "chart_joint = sns.jointplot(data=df, x=\"text_len\", y=\"word_count\", height=5)\n",
        "chart_joint.fig.suptitle(\"Correlation between word count and text lenght\")\n",
        "chart_joint.fig.tight_layout()\n",
        "chart_joint.fig.subplots_adjust(top=0.92)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoZe2yuG9B4u"
      },
      "outputs": [],
      "source": [
        "# Check word median per category\n",
        "df.groupby('problem')['word_count'].median()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWtuhOjN6UU"
      },
      "source": [
        "## Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezJirXNkaeVC"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d_Rtp_pCVpv"
      },
      "source": [
        "There are several 7B models available as open source, such as Mistral, LLama, Falcon, Zephyr, and Openchat. In this study we will use Mistral, but other models can also be tested, just paying attention to adapting the instruction structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi0-tycumPr4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta, datetime\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiKmBM3s4bEr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install vllm dependency\n",
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hYNAQcn4bH1"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNrC6Je7DLJ_"
      },
      "source": [
        "We will load the pre-trained Mistral 7B model, quantized by the user TheBloke and available at: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEZiXigZ4bK2"
      },
      "outputs": [],
      "source": [
        "# Load the LLM\n",
        "model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n",
        "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=60)\n",
        "model = LLM(\n",
        "    model=model_id,\n",
        "    quantization='awq',\n",
        "    dtype='half',\n",
        "    gpu_memory_utilization=.95,\n",
        "    max_model_len=4096\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE5bdD4xai9x"
      },
      "source": [
        "### Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqjCsmUSkzH"
      },
      "source": [
        "Prompt Guides:\n",
        "* https://docs.mistral.ai/guides/prompting-capabilities/\n",
        "* https://www.promptingguide.ai/models/mistral-7b\n",
        "* https://huggingface.co/docs/transformers/main/tasks/prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-WCnO807zU_"
      },
      "outputs": [],
      "source": [
        "# Define tags\n",
        "tags = ['sinal/conexão de rede', 'cobrança indevida', 'consumo saldo/crédito',\n",
        "        'plano/benefício', 'cancelamento linha/plano', 'chip/sim card', 'spam',\n",
        "        'portabilidade', 'recarga/pagamento', 'dificuldade de contato']\n",
        "\n",
        "# Transform tag list in one string\n",
        "tags_comma = \", \".join(f\"{tag}\" for tag in tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gJDFLcbD1VO"
      },
      "outputs": [],
      "source": [
        "# Examples\n",
        "dialog_dict = {\n",
        "    \"conv1_1t\": [\"Esse plano é ruim!\", \"plano/benefício\"],\n",
        "    \"conv2_1t\": [\"Recebo muitas mensagens de propaganda\", \"spam\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wHWn3uPULbT"
      },
      "outputs": [],
      "source": [
        "complaint_text = \"\"\"\n",
        "Reclamação: {user_complain}\n",
        "\"\"\"\n",
        "\n",
        "task = \"\"\"\n",
        "Tarefa: Classifique a reclamação. Atenção use apenas as categorias abaixo.\n",
        "{tags}\n",
        "\n",
        "Importante, apenas classifique sem explicar!\n",
        "\"\"\"\n",
        "answer = \"\"\"\n",
        "Rótulos:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_KS0hO40FsZ"
      },
      "source": [
        "#### Zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cY844Br0l3S"
      },
      "outputs": [],
      "source": [
        "zero_shot = ('[INST]' + complaint_text\n",
        "             + task.format(tags=tags_comma)\n",
        "             + answer + '[/INST]')\n",
        "print(zero_shot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmSYu0Tc0Q3C"
      },
      "source": [
        "#### Few-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6t13ucbGI5c"
      },
      "outputs": [],
      "source": [
        "few_examples = \"\"\"\n",
        "###\n",
        "Abaixo alguns exemplos:\n",
        "\n",
        "Reclamação: {ex1}\n",
        "Rótulos: {cat1}\n",
        "Reclamação: {ex2}\n",
        "Rótulos: {cat2}\n",
        "###\n",
        "\"\"\".format(ex1=dialog_dict['conv1_1t'][0], cat1=dialog_dict['conv1_1t'][1],\n",
        "           ex2=dialog_dict['conv2_1t'][0], cat2=dialog_dict['conv2_1t'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLaiOkHVXutp"
      },
      "outputs": [],
      "source": [
        "few_shot = (\n",
        "    '[INST]'\n",
        "    + complaint_text + task.format(tags=tags_comma)\n",
        "    + few_examples + answer\n",
        "    + '[/INST]')\n",
        "\n",
        "print(few_shot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXHHNdh80fJS"
      },
      "source": [
        "#### Multi-turn conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3sJFfGaoLeE"
      },
      "outputs": [],
      "source": [
        "conv = \"\"\"<s>[INST]\n",
        "Reclamação: {user_complain}\n",
        "\n",
        "Tarefa: Classifique a reclamação. Atenção use apenas as categorias abaixo.\n",
        "{tags}\n",
        "\n",
        "Importante, apenas classifique sem explicar!\n",
        "\n",
        "Rótulos:[/INST] {answer}\n",
        "</s>\n",
        "\"\"\"\n",
        "conv_1 = conv.format(user_complain=dialog_dict['conv1_1t'][0],\n",
        "                     tags=tags_comma,\n",
        "                     answer=dialog_dict['conv1_1t'][1])\n",
        "conv_2 = conv.format(user_complain=dialog_dict['conv2_1t'][0],\n",
        "                     tags=tags_comma,\n",
        "                     answer=dialog_dict['conv2_1t'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxd8AMoPpGTi"
      },
      "outputs": [],
      "source": [
        "multi_turn = (conv_1 + conv_2 + zero_shot)\n",
        "print(multi_turn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H0ViCXtLZds"
      },
      "source": [
        "#### Prompt test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSVo4EWqgzL-"
      },
      "outputs": [],
      "source": [
        "# Print each prompt (check)\n",
        "prompt_dict = {'zero_shot': zero_shot, 'few_shot': few_shot, 'multi_turn': multi_turn}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUQOjj737e9n"
      },
      "outputs": [],
      "source": [
        "def llm_analysis(prompt, complain):\n",
        "  response = model.generate(prompt.format(user_complain=complain), sampling_params, use_tqdm=False)\n",
        "  llm_classification = response[0].outputs[0].text.replace('.', '').lower().split('\\n')[0]\n",
        "\n",
        "  return llm_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr9kWm_8lU-j"
      },
      "outputs": [],
      "source": [
        "def iterate_over_interval(df, column, prompt, limit_char, start_row, end_row, save):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for nloop, index in enumerate(range(start_row, end_row)):\n",
        "    complaint_content = str(df.loc[index, column]).lower()\n",
        "\n",
        "    # Checkpoint\n",
        "    if (nloop + 1) % 100 == 0:\n",
        "      delta_time = str(timedelta(seconds=time.time() - start_time)).split('.')[0]\n",
        "      print(f\"{delta_time} Analysis number {nloop + 1}\")\n",
        "\n",
        "    # Limit text lengh\n",
        "    if len(complaint_content) > limit_char: complaint_content = complaint_content[:limit_char]+'\\n'\n",
        "\n",
        "    #print(prompt.format(user_complain=complaint_content))  # check prompt\n",
        "\n",
        "    # Text analysis\n",
        "    analysis = llm_analysis(prompt, complaint_content)\n",
        "    df.at[index, 'llm_class'] = analysis\n",
        "\n",
        "  # Save final result\n",
        "  delta_time = str(timedelta(seconds=time.time() - start_time)).split('.')[0]\n",
        "  print(f\"{nloop + 1} cases analyzed. Total execution time: {delta_time}\")\n",
        "\n",
        "  if save == True:\n",
        "    current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    file_name = 'analise_reclamacao_' + str(start_row) + '_' + str(end_row) + current_datetime + '.xlsx'\n",
        "    df.to_excel(file_name, index=False)\n",
        "\n",
        "  return df, file_name if save == True else df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZjd_eAWdFVS"
      },
      "outputs": [],
      "source": [
        "# Run the example few times to see model consistency\n",
        "example =\"\"\"como sempre, atendimento vago e sem nenhuma solução. estou a uma semana sem serviço na minha linha,\n",
        "tentando falar na central e só fico presa na ura ou não consigo chegar a lugar algum. detalhe que tudo isso\n",
        "precisa ser feito de outro celular já que não tenho sinal!!!!!!!!!!!!!não é a primeira vez que fico sem sinal\n",
        "esse ano pelo chip virtual e que sou impactada em trabalho e já estou nisso a 1 semana. a resposta da central\n",
        "foi: vá até uma loja física. eu tenho um chip virtual, qual o sentido de ir a uma loja física olhar um chip\n",
        "virtual que o supervisor que consegui falar nem sabia do que se tratava???? fora que minha [ultima ida a loja\n",
        "fisica fui cobrada por um chip dependente indevido que colocaram na minha conta e que nunca recebi reembolso.segue\n",
        "protocolo:*******070quero saber qual o desconto que terei em fatura nesses dias sem serviço e não tenho\n",
        "disponibilidade para ir a loja física por isso adquiri um chip virtual pela facilidade (que não existe do lado\n",
        "de vocês) nesse processo.a qualidade do serviço é cada dia pior e eu ainda me pergunto pq sou cliente de vocës.\n",
        "\"\"\".lower()\n",
        "\n",
        "for prompt in prompt_dict:\n",
        "  print('\\n - ', prompt)\n",
        "  for i in list(range(1, 4)):\n",
        "    analysis = llm_analysis(prompt_dict[prompt], example)\n",
        "    print(analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JauKAKmspRHH"
      },
      "source": [
        "### Multi-label Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9brvCeTplPg"
      },
      "source": [
        "In this section we will evaluate our prompt. To do that we labaled mannualy the **df_val** dataframe to use as comparision with model output.\n",
        "\n",
        "There are a few metrics to evaluate multi-label classification, in this work we gonna use `classification_report`from `sklearn`, that is a well known package wich give us manny metrics to analyse. If you you want to know more about it, there's a good article made by Aniruddha Karajgi, on https://towardsdatascience.com/evaluating-multi-label-classifiers-a31be83da6ea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar_auDgfT9G_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNtkFEZ06aJl"
      },
      "outputs": [],
      "source": [
        "# Creates binary classification columns and sum total classifications\n",
        "def binary_cols(dataframe, column, tags):\n",
        "  for tag in tags:\n",
        "      dataframe[tag] = dataframe[column].apply(lambda x: 1 if tag in x else 0)\n",
        "\n",
        "  dataframe[column + '_sum'] = dataframe[tags].sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZERNC0giOUZJ"
      },
      "source": [
        "**Note:** Depending on the amount of data, the classification process can take a few hours. Using the quantized model described above, with zero shot prompt and utilizing the free tier infrastructure of Colab, I noticed that Mistral 7B takes about an hour to classify 2.000 cases.\n",
        "\n",
        "Colab tends to disconnect due to inactivity. To prevent this, open your browser's inspector, go to the console tab, paste the code below, and press enter.\n",
        "\n",
        "`function ClickConnect(){\n",
        "    console.log(\"Working\");\n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8ToURtgqPRI"
      },
      "outputs": [],
      "source": [
        "# Open validation dataset tagged manually\n",
        "df_val_url = 'https://raw.githubusercontent.com/rdemarqui/tim_data_academy/main/data/df_validation.xlsx'\n",
        "df_val_labeled = pd.read_excel(df_val_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCR1jNbTJTu4"
      },
      "outputs": [],
      "source": [
        "# Check each prompt performance\n",
        "y_expected = df_val_labeled[tags]\n",
        "score_dict = {}\n",
        "file_name_dict = {}\n",
        "\n",
        "for prompt in prompt_dict:\n",
        "  print(\"\\n- \", prompt)\n",
        "  temp_df, file_name = iterate_over_interval(df=df, column=\"description\",\n",
        "                                             prompt=prompt_dict[prompt],\n",
        "                                             limit_char=2000, start_row=0,\n",
        "                                             end_row=202, save=True)\n",
        "\n",
        "  binary_cols(dataframe=temp_df, column='llm_class', tags=tags)\n",
        "\n",
        "  y_pred = temp_df[tags]\n",
        "\n",
        "  score_dict[prompt] = classification_report(y_expected, y_pred,\n",
        "                                             output_dict=True,\n",
        "                                             target_names=tags)\n",
        "  file_name_dict[prompt] = file_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58JTqQxPrBlt"
      },
      "source": [
        "There's some ways to agregate metrics such as `micro average`, `macro avarage`, `weighted average` and `samples average`. In this study we gonna use `samples avg`, wich was specifically designed for multilabel scenarios. It calculates metrics like precision, recall, and F1-score for each instance individually and averages them, thus effectively evaluating the model's performance on each sample by considering all its labels, making it particularly useful for assessing how well the model predicts the label set for each individual sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxLYxxWZv0ME"
      },
      "outputs": [],
      "source": [
        "# Create score comparison table\n",
        "df_score_comparison = pd.DataFrame()\n",
        "\n",
        "for prompt in prompt_dict:\n",
        "  score_temp = score_dict[prompt]['samples avg']\n",
        "  df_temp = pd.DataFrame(score_temp, index=[prompt])\n",
        "  df_score_comparison = pd.concat([df_score_comparison, df_temp])\n",
        "\n",
        "round(df_score_comparison, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBREo3Ex-QTW"
      },
      "source": [
        "Remembering that:\n",
        "\n",
        "* **Precision:** It is the proportion of positive identifications that were actually correct. A low precision indicates that there were **too many False Positives (FP)**, meaning that the model predicted many instances as positive that were actually negative.\n",
        "\n",
        " $\\frac{\\text{True Positive}}{\\text{Total Predicted Positive}} = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP)} + \\text{False Positive (FP)}}$\n",
        "\n",
        "* **Recall:** It is the proportion of actual positives that were identified correctly. A low recall indicates that there were **too many False Negatives (FN)**, meaning that the model failed to identify many actual positive instances.\n",
        "\n",
        " $\\frac{\\text{True Positive}}{\\text{Total Actual Positive}} = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP)} + \\text{False Negative (FN)}}$\n",
        "\n",
        "* **F1 Score:** Is the harmonic mean of the precision and recall, bringing a good balance between both.\n",
        "\n",
        " ${\\text{F1}} = 2* \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}}$\n",
        "\n",
        "In this study we will use the prompt that achived the max F1 score, that represents a good balance between precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhuJNcTl-sXZ"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXmGFR_OmOlW"
      },
      "outputs": [],
      "source": [
        "max_score_prompt = df_score_comparison['f1-score'].idxmax()\n",
        "print(f\"The max score obtained was achived by '{max_score_prompt}' prompt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_dxrDY6_EZ1"
      },
      "outputs": [],
      "source": [
        "# Check detailed score from choosen prompt\n",
        "round(pd.DataFrame(score_dict[max_score_prompt]).transpose(), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUKGKT7JoNDS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sigbMhFcYBep"
      },
      "outputs": [],
      "source": [
        "report = classification_report(y_expected, y_pred, target_names=tags)\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}