{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdemarqui/llm_complaint_management/blob/main/02.%20LLM_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM at Work: Decoding Customer Feedback"
      ],
      "metadata": {
        "id": "JdXiJzMm7ewB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This study aims to explore the advanced LLM tool Mistral 7B to analyze a vast collection of complaints gathered from the website [reclameaqui.com.br](reclameaqui.com.br). Through this analysis, we intend to demonstrate the potential and effectiveness of LLMs in interpreting and handling large-scale user feedback.\n",
        "\n",
        "For more information, please visit github page: https://github.com/rdemarqui/llm_complaint_management"
      ],
      "metadata": {
        "id": "1Ex4zf24D1-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complaint Dataset"
      ],
      "metadata": {
        "id": "e_s1QiOU98Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load"
      ],
      "metadata": {
        "id": "kZWO80q3-PsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "sns.set_theme()"
      ],
      "metadata": {
        "id": "tSvJsvLbHoQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "data_url = 'https://raw.githubusercontent.com/rdemarqui/llm_complaint_management/main/datasets/full_dataset_claro.xlsx'\n",
        "data = pd.read_excel(data_url, usecols=['problem', 'description'])\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "cduSRhNh4VB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean"
      ],
      "metadata": {
        "id": "pAx7QfsGBLfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove extra space and uncase all words\n",
        "data['description'] = data['description'].astype(str)\n",
        "data['description'] = data['description'].str.replace(r'\\s{2,}', ' ', regex=True)\n",
        "data['description'] = data['description'].str.lower()"
      ],
      "metadata": {
        "id": "UmvxMuJuBOy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify mask made by reclame aqui\n",
        "def remove_mask(text):\n",
        "  return text.replace(\"[editado pelo reclame aqui]\", \"[mask]\")\n",
        "\n",
        "data['description'] = data['description'].apply(remove_mask)"
      ],
      "metadata": {
        "id": "mSHPHLv0JdLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove description less than 3 characters\n",
        "data = data[data['description'].str.len() > 3].copy()"
      ],
      "metadata": {
        "id": "n9DYRkEGdB5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratify"
      ],
      "metadata": {
        "id": "m7LKh33Oe5eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this work is intended for study purposes, we will only work with a fraction of the data. With the infrastructure provided by the Colab free tier, we noticed that on average, Mistral 7B analyzes about 2.000 complaints per hour. Therefore, we will use this number of cases in our analysis. To do this, we will perform a stratified sample.\n",
        "\n",
        "We also will use validation data with 202 examples to test some prompt engineering."
      ],
      "metadata": {
        "id": "qPR19OFje_TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get stratified data\n",
        "df, data_val = train_test_split(data, train_size=2002, stratify=data['problem'], random_state=42)\n",
        "df_val, _  = train_test_split(data_val, train_size=202, stratify=data_val['problem'], random_state=42)\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "df_val.reset_index(inplace=True, drop=True)\n",
        "print(f\"Validation data: {df_val.shape} \\nTest data: {df.shape}\")"
      ],
      "metadata": {
        "id": "KMLxpZ5hl4tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export dataset to tag mannualy\n",
        "df_val.to_excel('df_validation.xlsx', index=False)"
      ],
      "metadata": {
        "id": "E8CIQ22yqstY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing word distributions\n",
        "data['word_count'] = data['description'].str.split().str.len()\n",
        "df['word_count'] = df['description'].str.split().str.len()\n",
        "df_val['word_count'] = df_val['description'].str.split().str.len()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "sns.kdeplot(data=data['word_count'], label='Full dataset 7000 samples', fill=True, ax=ax)\n",
        "sns.kdeplot(data=df_val['word_count'], label='Validation dataset 202 samples', fill=True, ax=ax)\n",
        "sns.kdeplot(data=df['word_count'], label='Test dataset 2002 samples', fill=True, ax=ax)\n",
        "ax.legend()\n",
        "plt.title('Words Quantity Comparison in Stratified Datasets')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uuoynKmm5AcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "RlSuXm0N-APD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "kximFpZZpSed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show each problem category\n",
        "df['problem'].value_counts()"
      ],
      "metadata": {
        "id": "trUhFZEarsl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show words statistics\n",
        "df['word_count'] = df['description'].str.split().str.len()\n",
        "df['text_len'] = df['description'].str.len()\n",
        "df['word_count'].describe()"
      ],
      "metadata": {
        "id": "G0gvCGaxsuBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check correlation between word count and text lenght\n",
        "chart_joint = sns.jointplot(data=df, x=\"text_len\", y=\"word_count\", height=5)\n",
        "chart_joint.fig.suptitle(\"Correlation between word count and text lenght\")\n",
        "chart_joint.fig.tight_layout()\n",
        "chart_joint.fig.subplots_adjust(top=0.92)"
      ],
      "metadata": {
        "id": "KMrNKNN4vdjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check word median per category\n",
        "df.groupby('problem')['word_count'].median()"
      ],
      "metadata": {
        "id": "GoZe2yuG9B4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification"
      ],
      "metadata": {
        "id": "0pWtuhOjN6UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Model"
      ],
      "metadata": {
        "id": "ezJirXNkaeVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several 7B models available as open source, such as Mistral, Falcon, Zephyr, and Openchat. In this study we will use Mistral, but other models can also be tested, just paying attention to adapting the instruction structure."
      ],
      "metadata": {
        "id": "5d_Rtp_pCVpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "Bi0-tycumPr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latest HF transformers version for Mistral-like models\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# GPTQ Dependencies\n",
        "!pip install optimum\n",
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "NLHr_hhG0DyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "gJCc0MNL0D1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load the pre-trained Mistral 7B model, quantized by the user TheBloke and available at https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ.\n",
        "\n",
        "For more information on quantization, there is an excellent article written by Maxime Labonne:\n",
        "* https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c"
      ],
      "metadata": {
        "id": "LNrC6Je7DLJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM and Tokenizer\n",
        "model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False,\n",
        "    revision=\"main\"\n",
        ")\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "4HxNvdf90D4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "generator = pipeline(model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                task='text-generation',\n",
        "                max_new_tokens=60,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                return_full_text = False\n",
        "                )\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "aW5V7xEs3Vne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering"
      ],
      "metadata": {
        "id": "LE5bdD4xai9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_analysis(prompt, complain):\n",
        "  response = generator(prompt.format(user_complain=complain))\n",
        "  llm_classification = response[0]['generated_text'].replace('.', '').lower()\n",
        "\n",
        "  return llm_classification"
      ],
      "metadata": {
        "id": "rLWWtjhppGl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_over_interval(df, column, prompt, limit_char, start_row, end_row, save):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for nloop, index in enumerate(range(start_row, end_row)):\n",
        "    complaint_content = str(df.loc[index, column]).lower()\n",
        "\n",
        "    # Checkpoint\n",
        "    if (nloop + 1) % 100 == 0:\n",
        "      delta_time = str(timedelta(seconds=time.time() - start_time)).split('.')[0]\n",
        "      print(f\"{delta_time} Analysis number {nloop + 1}\")\n",
        "\n",
        "    # Limit text lengh\n",
        "    if len(complaint_content) > limit_char: complaint_content = complaint_content[:limit_char]+'\\n'\n",
        "\n",
        "    #print(prompt.format(user_complain=complaint_content))  # check prompt\n",
        "\n",
        "    # Text analysis\n",
        "    analysis = llm_analysis(prompt, complaint_content)\n",
        "    df.at[index, 'llm_class'] = analysis\n",
        "\n",
        "  # Save final result\n",
        "  delta_time = str(timedelta(seconds=time.time() - start_time)).split('.')[0]\n",
        "  print(f\"{nloop + 1} cases analyzed. Total execution time: {delta_time}\")\n",
        "\n",
        "  if save == True:\n",
        "    file_save = 'analise_reclamacao_' + str(start_row) + '_' + str(end_row) + '.xlsx'\n",
        "    df.to_excel(file_save, index=False)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Wr9kWm_8lU-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Guide:\n",
        "* https://www.promptingguide.ai/models/mistral-7b\n",
        "* https://huggingface.co/docs/transformers/main/tasks/prompting"
      ],
      "metadata": {
        "id": "xAqjCsmUSkzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tags\n",
        "tags = ['sinal/conexão de rede', 'cobrança indevida', 'consumo saldo/crédito',\n",
        "        'plano/benefício', 'cancelamento linha/plano', 'chip/sim card', 'spam',\n",
        "        'portabilidade', 'recarga/pagamento', 'dificuldade de contato']\n",
        "\n",
        "# Transform tag list in one string\n",
        "tags_string = \", \".join(f\"{tag}\" for tag in tags)\n",
        "\n",
        "tags_string"
      ],
      "metadata": {
        "id": "9-WCnO807zU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-turn conversation\n",
        "\n",
        "# Conversation 1 with 1 example\n",
        "conv1_1e = \"\"\"[INST]\n",
        "Reclamação: A cobertura é ruim!\n",
        "Tarefa: Classifique a reclamação.\n",
        "\n",
        "Rótulos:[/INST] sinal/conexão de rede\n",
        "\"\"\"\n",
        "\n",
        "# Conversation 1 with 2 examples\n",
        "conv1_2e = \"\"\"[INST]\n",
        "Reclamação: A cobertura é ruim! Meu chip não funciona!\n",
        "Tarefa: Classifique a reclamação.\n",
        "\n",
        "Rótulos:[/INST] sinal/conexão de rede, chip/sim card\n",
        "\"\"\"\n",
        "\n",
        "# Conversation 2 with 1 example\n",
        "conv2_1e = \"\"\"[INST]\n",
        "Reclamação: Recebo muitas mensagens de propaganda.\n",
        "Tarefa: Classifique a reclamação.\n",
        "\n",
        "Rótulos:[/INST] spam\n",
        "\"\"\"\n",
        "\n",
        "# Conversation 2 with 2 examples\n",
        "conv2_2e = \"\"\"[INST]\n",
        "Reclamação: Recebo muitas mensagens de propaganda. Meu saldo acaba rápido.\n",
        "Tarefa: Classifique a reclamação.\n",
        "\n",
        "Rótulos:[/INST] spam, consumo saldo/crédito\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w8HkBsK6gW58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complain_template = \"\"\"\n",
        "Reclamação: {user_complain}\"\"\"\n",
        "\n",
        "task_template = \"\"\"\n",
        "Tarefa: Classifique a reclamação. Atenção use apenas as categorias abaixo, sem explicar.\n",
        "{tags}\"\"\".format(tags=tags_string)"
      ],
      "metadata": {
        "id": "6wQGSoxCDQl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt user complain without system prompt (role)\n",
        "prompt_task_after = '[INST]' + complain_template + '\\n' + task_template + '\\n\\nRótulos:[/INST]'\n",
        "prompt_task_before = '[INST]' + task_template + '\\n' + complain_template + '\\n\\nRótulos:[/INST]'"
      ],
      "metadata": {
        "id": "nV9GLEVP8md_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt zero shot\n",
        "p_tsk_aft_zero = prompt_task_after\n",
        "p_tsk_bfr_zero = prompt_task_before\n",
        "\n",
        "# Prompt multi-turn conversation, task after complain\n",
        "p_tsk_aft_1c_1e = '<s>' + conv1_1e + '</s>\\n' + prompt_task_after\n",
        "p_tsk_aft_2c_1_2e = '<s>' + conv1_1e + conv2_2e + '</s>\\n' + prompt_task_after\n",
        "p_tsk_aft_1c_2e = '<s>' + conv1_2e + '</s>\\n' + prompt_task_after\n",
        "p_tsk_aft_2c_2e = '<s>' + conv1_2e + conv2_2e + '</s>\\n' + prompt_task_after\n",
        "\n",
        "# Prompt multi-turn conversation, task before complain\n",
        "p_tsk_bfr_1c_1e = '<s>' + conv1_1e + '</s>\\n' + prompt_task_before\n",
        "p_tsk_bfr_2c_1_2e = '<s>' + conv1_1e + conv2_2e + '</s>\\n' + prompt_task_before\n",
        "p_tsk_bfr_1c_2e = '<s>' + conv1_2e + '</s>\\n' + prompt_task_before\n",
        "p_tsk_bfr_2c_2e = '<s>' + conv1_2e + conv2_2e + '</s>\\n' + prompt_task_before\n",
        "\n",
        "prompt_dict = {'p_tsk_aft_zero': p_tsk_aft_zero, 'p_tsk_bfr_zero': p_tsk_bfr_zero,\n",
        "               'p_tsk_aft_1c_1e': p_tsk_aft_1c_1e, 'p_tsk_aft_2c_1_2e': p_tsk_aft_2c_1_2e,\n",
        "               'p_tsk_aft_1c_2e': p_tsk_aft_1c_2e, 'p_tsk_aft_2c_2e': p_tsk_aft_2c_2e,\n",
        "               'p_tsk_bfr_1c_1e': p_tsk_bfr_1c_1e, 'p_tsk_bfr_2c_1_2e': p_tsk_bfr_2c_1_2e,\n",
        "               'p_tsk_bfr_1c_2e': p_tsk_bfr_1c_2e, 'p_tsk_bfr_2c_2e': p_tsk_bfr_2c_2e}"
      ],
      "metadata": {
        "id": "xSVo4EWqgzL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print each prompt (check)\n",
        "#for prompt in prompt_dict:\n",
        "#  print('\\n - ', prompt)\n",
        "#  print(prompt_dict[prompt])"
      ],
      "metadata": {
        "id": "cUgpc_mdl0Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're gonna try many prompt examples, as described int table below:\n",
        "\n",
        "|Prompt name|Task|Shot|Label|\n",
        "|--|--|--|--|\n",
        "|p_tsk_aft_zero|After complaint|zero|-|\n",
        "|p_tsk_bfr_zero|Before complaint|zero|-|\n",
        "|p_tsk_aft_1c_1e|After complaint|one|1|\n",
        "|p_tsk_aft_2c_1_2e|After complaint|two|1 and 2|\n",
        "|p_tsk_aft_1c_2e|After complaint|one|2|\n",
        "|p_tsk_aft_2c_2e|After complaint|two|2|\n",
        "|p_tsk_bfr_1c_1e|Before complaint|one|1|\n",
        "|p_tsk_bfr_2c_1_2e|Before complaint|two|1 and 2|\n",
        "|p_tsk_bfr_1c_2e|Before complaint|one|2|\n",
        "|p_tsk_bfr_2c_2e|Before complaint|two|2|"
      ],
      "metadata": {
        "id": "aOyGO5YEjMZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the example few times to see model consistency\n",
        "example =\"\"\"\n",
        "como sempre, atendimento da claro vago e sem nenhuma solução. estou a uma semana sem serviço na minha linha, tentando falar na central e só fico presa na ura ou não consigo chegar a lugar algum. detalhe que tudo isso precisa ser feito de outro celular já que não tenho sinal!!!!!!!!!!!!!não é a primeira vez que fico sem sinal esse ano pelo chip virtual e que sou impactada em trabalho e já estou nisso a 1 semana. a resposta da central foi: vá até uma loja física. eu tenho um chip virtual, qual o sentido de ir a uma loja física olhar um chip virtual que o supervisor que consegui falar nem sabia do que se tratava???? fora que minha [ultima ida a loja fisica fui cobrada por um chip dependente indevido que colocaram na minha conta e que nunca recebi reembolso.segue protocolo:*******070quero saber qual o desconto que terei em fatura nesses dias sem serviço e não tenho disponibilidade para ir a loja física por isso adquiri um chip virtual pela facilidade (que não existe do lado de vocês) nesse processo.a qualidade do serviço é cada dia pior e eu ainda me pergunto pq sou cliente de vocës.\n",
        "\"\"\"[:2000].lower()\n",
        "\n",
        "for prompt in prompt_dict:\n",
        "  print('\\n - ', prompt)\n",
        "  for i in list(range(1, 4)):\n",
        "    analysis = llm_analysis(prompt_dict[prompt], example)\n",
        "    print(analysis)"
      ],
      "metadata": {
        "id": "YZjd_eAWdFVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** As we can be seen below, model tends to use more labels wen we pass multi-turn conversations with 2 exemple labels."
      ],
      "metadata": {
        "id": "kTjb1jNadpv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-label Evaluation"
      ],
      "metadata": {
        "id": "JauKAKmspRHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will evaluate our prompt. To do that we labaled mannualy the **df_val** dataframe to use as comparision with model output.\n",
        "\n",
        "There are a few metrics to evaluate multi-label classification, in this work we gonna use `classification_report`from `sklearn`, that is a well known package wich give us manny metrics to analyse. If you you want to know more about it, there's a good article made by Aniruddha Karajgi, on https://towardsdatascience.com/evaluating-multi-label-classifiers-a31be83da6ea."
      ],
      "metadata": {
        "id": "p9brvCeTplPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ar_auDgfT9G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates binary classification columns and sum total classifications\n",
        "def binary_cols(dataframe, column, tags):\n",
        "  for tag in tags:\n",
        "      dataframe[tag] = dataframe[column].apply(lambda x: 1 if tag in x else 0)\n",
        "\n",
        "  dataframe[column + '_sum'] = dataframe[tags].sum(axis=1)"
      ],
      "metadata": {
        "id": "JNtkFEZ06aJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Depending on the amount of data, the classification process can take a few hours. Using the quantized model described above and utilizing the free tier infrastructure of Colab, I noticed that Mistral 7B takes about an hour to classify 2.000 cases.\n",
        "\n",
        "Colab tends to disconnect due to inactivity. To prevent this, open your browser's inspector, go to the console tab, paste the code below, and press enter.\n",
        "\n",
        "`function ClickConnect(){\n",
        "    console.log(\"Working\");\n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)`"
      ],
      "metadata": {
        "id": "ZERNC0giOUZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open validation dataset tagged manually\n",
        "df_val_url = 'https://raw.githubusercontent.com/rdemarqui/llm_complaint_management/main/datasets/df_validation.xlsx'\n",
        "df_val_labeled = pd.read_excel(df_val_url)"
      ],
      "metadata": {
        "id": "l8ToURtgqPRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check each prompt performance\n",
        "y_expected = df_val_labeled[tags]\n",
        "score_dict = {}\n",
        "\n",
        "for prompt in prompt_dict:\n",
        "\n",
        "  print(\"- \", prompt)\n",
        "  temp = iterate_over_interval(df=df_val, column=\"description\", prompt = prompt_dict[prompt],\n",
        "                               limit_char=2000, start_row=0, end_row=df_val.shape[0], save=False)\n",
        "\n",
        "  binary_cols(dataframe=temp, column='llm_class', tags=tags)\n",
        "\n",
        "  y_pred = temp[tags]\n",
        "\n",
        "  score_dict[prompt] = classification_report(y_expected, y_pred, output_dict=True, target_names=tags)"
      ],
      "metadata": {
        "id": "bCR1jNbTJTu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_expected = df_val_labeled[tags]\n",
        "#y_pred = y_expected\n",
        "#df = pandas.DataFrame(report).transpose()\n",
        "#pd.DataFrame(report_dict)"
      ],
      "metadata": {
        "id": "JXahcbK0qPdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score_comparison = pd.DataFrame()\n",
        "\n",
        "for prompt in prompt_dict:\n",
        "  score_temp = score_dict[prompt]['samples avg']\n",
        "  df_temp = pd.DataFrame(score_temp, index=[prompt])\n",
        "  df_score_comparison = pd.concat([df_score_comparison, df_temp])\n",
        "\n",
        "round(df_score_comparison, 3)"
      ],
      "metadata": {
        "id": "oxLYxxWZv0ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remembering that:\n",
        "\n",
        "* **Precision:** Proportion of positive identifications that was actually correct. A low precision means **too many False Positives (FP)**.\n",
        "\n",
        " $\\frac{\\text{True Positive}}{\\text{Total Predicted Positive}} = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP)} + \\text{False Positive (FP)}}$\n",
        "\n",
        "* **Recall:** Proportion of actual positives that was identified correctly. A low recall means **too many False Negatives (FN)**.\n",
        "\n",
        " $\\frac{\\text{True Positive}}{\\text{Total Actual Positive}} = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP)} + \\text{False Negative (FN)}}$\n",
        "\n",
        "* **F1 Score:** Is the harmonic mean of the precision and recall, bringing a good balance between both.\n",
        "\n",
        " ${\\text{F1}} = 2* \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}}$"
      ],
      "metadata": {
        "id": "KBREo3Ex-QTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this study we will use the prompt that achived the max F1 score."
      ],
      "metadata": {
        "id": "zJeirEGPjp1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_score_prompt = df_score_comparison['f1-score'].idxmax()\n",
        "print(f\"The max score obtained was achived by '{max_score_prompt}' prompt.\")"
      ],
      "metadata": {
        "id": "WXmGFR_OmOlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check detailed score form choosen prompt\n",
        "pd.DataFrame(score_dict[max_score_prompt]).transpose()"
      ],
      "metadata": {
        "id": "r_dxrDY6_EZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "09uTIRku-kRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** As can be seen in the analysis above, the complaints have a median of 86 words and a text length with a median of 500 characters. Few texts exceed 2000 characters, for this reason, we will limit the text size to be passed to the model to this value on `limit_char` parameter."
      ],
      "metadata": {
        "id": "JKF6BRMcptsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run text classification\n",
        "iterate_over_interval(df=df, column=\"description\", prompt=prompt_dict[max_score_prompt]\n",
        "                      limit_char=2000, start_row=0, end_row=end_row=df.shape[0], save=True)"
      ],
      "metadata": {
        "id": "LqfD3bzLl01G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "nhuJNcTl-sXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create binary classification columns\n",
        "binary_cols(df, 'llm_class')"
      ],
      "metadata": {
        "id": "lszAyhCcmR_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the quantity os texts that weren't classified using at least one of tags\n",
        "unknow_tags = (df['llm_class_sum'] == 0).sum()\n",
        "print(f\"There are {unknow_tags} texts classified without using at least one of correct tags (hallucination).\")"
      ],
      "metadata": {
        "id": "Qwf4cEYi2ZV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class quantity\n",
        "df_class = df[tags]\n",
        "class_qty = df_class.sum()\n",
        "\n",
        "chart_class = class_qty.sort_values(ascending=False)\n",
        "sns.barplot(x=chart_class.values, y=chart_class.index, orient='h', palette='Blues_r')\n",
        "plt.title('Class quantity');"
      ],
      "metadata": {
        "id": "kw7sj0SqklwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_matrix = round((df.groupby('problem')[tags].sum().T / df['problem'].value_counts().sort_index()), 2).T\n",
        "sns.heatmap(freq_matrix, annot=True, cmap='Blues')\n",
        "plt.title('Frequency Matrix of Tagged Problems');"
      ],
      "metadata": {
        "id": "l61F2lX_463l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "fUKGKT7JoNDS"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}